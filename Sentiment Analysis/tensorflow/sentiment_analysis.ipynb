{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie reviews - LSTM sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini project we will implement model for sentiment analysis, based on movie reviews from IMDB, we will predict sentiment (positive/negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAR_FILE_NAME = 'ImdbReviews.tar.gz'\n",
    "DIR_NAME = \"aclImdb\"\n",
    "URL_PATH = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download and unpack a dataset provided by Stanford Artificial Intelligence Laboratory.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    if not os.path.exists(DIR_NAME):\n",
    "        file, _ = urllib.request.urlretrieve(url, TAR_FILE_NAME)\n",
    "        with tarfile.open(TAR_FILE_NAME) as tar:\n",
    "            tar.extractall()\n",
    "            tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look over the each txt file, and extract text into np.array. Postivie reviews are in the directory: \"/train/pos/\", and negative in: \"/train/neg/\". When we will have extracted and labeled reviews, we will shuffle them to have a better disribution. \n",
    "Let's define our function for extraction and shuffling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_from_files(dirname, positive = True):\n",
    "    label = 1 if positive else 0\n",
    "    \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(dirname + filename, 'r+', encoding=\"utf8\", ) as f:\n",
    "                review = f.read()\n",
    "                review = review.lower().replace(\"<br />\", \" \")\n",
    "                \n",
    "                reviews.append(review)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    np.random.seed(1)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(x)))\n",
    "    \n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    \n",
    "    return x_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass text data into LSTM, we will have to create a vocabulary and map sequence of words into sequence of id's of this words from our vocabulary.\n",
    "To do this we will use VocabularyProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    pos_reviews, pos_labels = get_reviews_from_files(DIR_NAME + \"/train/pos/\", positive = True)\n",
    "    neg_reviews, neg_labels = get_reviews_from_files(DIR_NAME + \"/train/neg/\", positive = False)\n",
    "    \n",
    "    labels =np.array(pos_labels + neg_labels)\n",
    "    data = pos_reviews + neg_reviews\n",
    "    \n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_SEQUENCE_LENGTH)\n",
    "    data = np.array(list(vocab_processor.fit_transform(data)))\n",
    "    data, labels = shuffle(data, labels)\n",
    "    return data, labels, len(vocab_processor.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use ur helper function to prepare dataset and split it into training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(URL_PATH)\n",
    "x_train, y_train, vocabulary_size = get_data()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement our LSTM model, we will define 3 layers:\n",
    "    - Embedding layer, we will use 64 length vectors\n",
    "    - LSTM layer wirh 64 memory units, and 0.7 keep_prop\n",
    "    - Dense layer as an output layer with softmax activation function and only one output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 25\n",
    "embedding_size = 64\n",
    "max_label = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholder, which will be an input and output of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, MAX_SEQUENCE_LENGTH])\n",
    "y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define embedding matrix and embedding vecotr for each word in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embeddings = tf.nn.embedding_lookup(embedding_matrix, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LSTM cell, set input size owhich will be our embedding vector size, and add a dropout with 0.7 keep propability after LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(embedding_size)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our lstm network with define lstm cell and embeddings. As the output layer we will define dense layer with two labels (positive/ negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (encoding, _) = tf.nn.dynamic_rnn(lstmCell, embeddings, dtype= tf.float32)\n",
    "logits = tf.layers.dense(encoding, max_label, activation = None)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define our loss function, optimizer and a train step which should minimize value of our loss function.\n",
    "Additionally we will define accuracy and a prediction so we could messure performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "prediction = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement our training session. In for loop we will perform mini batch training and we will repeat training for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test Loss: 0.62, Test Acc: 0.7378\n",
      "Epoch: 2, Test Loss: 0.47, Test Acc: 0.7954\n",
      "Epoch: 3, Test Loss: 0.5, Test Acc: 0.806\n",
      "Epoch: 4, Test Loss: 0.66, Test Acc: 0.8054\n",
      "Epoch: 5, Test Loss: 0.76, Test Acc: 0.7978\n",
      "Epoch: 6, Test Loss: 0.8, Test Acc: 0.7944\n",
      "Epoch: 7, Test Loss: 0.97, Test Acc: 0.8028\n",
      "Epoch: 8, Test Loss: 0.93, Test Acc: 0.7996\n",
      "Epoch: 9, Test Loss: 1.4, Test Acc: 0.799\n",
      "Epoch: 10, Test Loss: 1.2, Test Acc: 0.7956\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        num_batches = int(len(x_train)//batch_size) +1\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            min_batch_x = batch * batch_size\n",
    "            max_batch_x = np.min([len(x_train), ((batch+1) * batch_size)])\n",
    "           \n",
    "            x_train_batch = x_train[min_batch_x:max_batch_x]\n",
    "            y_train_batch = y_train[min_batch_x:max_batch_x]\n",
    "            \n",
    "            train_dict = {x: x_train_batch, y: y_train_batch}\n",
    "            session.run(train_step, feed_dict = train_dict)\n",
    "            \n",
    "            train_loss, train_acc = session.run([loss, accuracy], feed_dict = train_dict)\n",
    "            \n",
    "            \n",
    "        \n",
    "        test_dict = {x: x_test, y: y_test}\n",
    "        \n",
    "        test_loss, test_acc = session.run([loss, accuracy], feed_dict=test_dict)\n",
    "        print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.5}'.format(epoch + 1, test_loss, test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
