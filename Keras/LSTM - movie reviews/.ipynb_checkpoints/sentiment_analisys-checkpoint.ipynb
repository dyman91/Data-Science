{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie reviews - LSTM sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this mini project we will implement model for sentiment analysis, based on movie reviews from IMDB, we will predict sentiment (positive/negative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras's imbd dataset of 25,000 reviews. Reviews have been preprocessed and each review is encoded as sequence of word indexes. To see how reviews really looks like we will have to implement an dencoder from index into actual word. Based on dataset description first indexes has special meaning:\n",
    "     - 0: this index will be used for padding\n",
    "     - 1: start sign\n",
    "     - 2: unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 1\n",
      "encoded word sequence: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "decoded word sequence: <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "num_words = 6000        \n",
    "max_review_len = 100    \n",
    "batch_size = 24\n",
    "epochs = 5\n",
    "index_from = 3\n",
    "\n",
    "def get_decoder():\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "    word_to_id[\"<END>\"] = 3\n",
    "    id_to_word = {value:key for key, value in word_to_id.items()}\n",
    "    \n",
    "    return id_to_word\n",
    "\n",
    "def print_example(index, id_to_word):\n",
    "    print(\"class:\", y_train[index])\n",
    "    print(\"encoded word sequence:\", x_train[index])\n",
    "    print(\"decoded word sequence:\", ' '.join(id_to_word[id] for id in x_train[index]))\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = num_words, index_from=index_from)\n",
    "decoder = get_decoder()\n",
    "print_example(0, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras dataset provide an argument for skipping most common words. We will use it to exclude words like : \"the\", \"is\", \"a\" etc. which brings no value to our model. Lets print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 the\n",
      "2 and\n",
      "3 a\n",
      "4 of\n",
      "5 to\n",
      "6 is\n",
      "7 br\n",
      "8 in\n",
      "9 it\n",
      "10 i\n",
      "11 this\n",
      "12 that\n",
      "13 was\n",
      "14 as\n",
      "15 for\n",
      "16 with\n",
      "17 movie\n",
      "18 but\n",
      "19 film\n",
      "20 on\n"
     ]
    }
   ],
   "source": [
    "def print_most_frequent_words(num_words, id_to_word):\n",
    "    for i in range(4, num_words + 4):\n",
    "        print(i-3, id_to_word[i])\n",
    "\n",
    "print_most_frequent_words(20, get_decoder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on rule of thumb :) lets skip first 15 words, to do that we will load dataset once again. Additionally we will perform padding on samples to fit int our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(skip_top, max_review_len):\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = num_words, index_from=index_from, skip_top = skip_top)\n",
    "    #   Pad and truncate the review word sequences so they are all the same length\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen = max_review_len)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen = max_review_len)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_data(15, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement our LSTM model. We will use sequential mode and define 3 layers:\n",
    "    - Embedding layer, we will use 64 length vectors\n",
    "    - LSTM layer wirh 64 memory units, and 0.3 dropout\n",
    "    - Dense layer as an output layer with sigmoid activation function and only one output neuron\n",
    "\n",
    "Because it's a binary classification problem we will use binary_crossentropy as los function. For optimizition we will use effective adam method. To avoid overfitting we will use early stopping technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 109s 4ms/step - loss: 0.4963 - acc: 0.7584 - val_loss: 0.4300 - val_acc: 0.8040\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 98s 4ms/step - loss: 0.3809 - acc: 0.8379 - val_loss: 0.4418 - val_acc: 0.7887\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 95s 4ms/step - loss: 0.3298 - acc: 0.8635 - val_loss: 0.3891 - val_acc: 0.8244\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 97s 4ms/step - loss: 0.2984 - acc: 0.8776 - val_loss: 0.3775 - val_acc: 0.8367\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 96s 4ms/step - loss: 0.2615 - acc: 0.8942 - val_loss: 0.3944 - val_acc: 0.8348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca3d0e3dd8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 64 ))\n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#   Compile\n",
    "model.compile(loss='binary_crossentropy',  \n",
    "            optimizer='adam',              \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "#   Train\n",
    "cbk_early_stopping = EarlyStopping(monitor='val_acc', patience=2, mode='max')\n",
    "model.fit(x_train, y_train, batch_size, epochs=epochs, \n",
    "            validation_data=(x_test, y_test), \n",
    "            callbacks=[cbk_early_stopping] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets check the performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 14s 549us/step\n",
      "test score: 0.3944254387807846  test accuracy: 0.834800000371933\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('test score:', score, ' test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
