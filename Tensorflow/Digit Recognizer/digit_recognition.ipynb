{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of CNN network for digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this micro-project, we will implement Convolutional Neural Network fo digit recognition in use of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 28\n",
    "image_height = 28\n",
    "num_classes = 10\n",
    "learning_rate = 0.002\n",
    "epochs = 6\n",
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading input data, data will be returned as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "def load_data():\n",
    "    train_set = pd.read_csv('data/train.csv')\n",
    "    test_set = pd.read_csv('data/test.csv')\n",
    "\n",
    "    x_train = train_set.iloc[:, 1:].values\n",
    "    y_train = train_set.iloc[:, 0].values\n",
    "    x_test = test_set.iloc[:,:].values\n",
    "    \n",
    "    return x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for converting array into one hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array into one hot matrix\n",
    "def convert_to_one_hot(arr):\n",
    "    one_hot = np.zeros((arr.size, arr.max() + 1))\n",
    "    one_hot[np.arange(arr.size), arr] = 1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for normalizing data values from input range (1 - 255) into target range (0. - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    data = data/data.max()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for saving predictions into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(preds):\n",
    "    y_test = preds.astype(int)\n",
    "    csv_content = pd.DataFrame({'ImageId': range(1,len(y_test)+1), 'Label': y_test})\n",
    "    csv_content.to_csv('result.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to create weights, biases, convolution and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def conv(x, W, name=None):\n",
    "    return tf.nn.conv2d(x,W, strides=[1,1,1,1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool(x, name=None):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function display an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img):\n",
    "    \n",
    "    # (784) => (28,28)\n",
    "    one_image = img.reshape(image_height,image_width)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(one_image, cmap=cm.binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Function to saving predictions into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(preds):\n",
    "    y_test = preds.astype(int)\n",
    "    csv_content = pd.DataFrame({'ImageId': range(1,len(y_test)+1), 'Label': y_test})\n",
    "    csv_content.to_csv('result.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32, shape = [None, image_height*image_width], name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, shape = [None, num_classes], name=\"y\")\n",
    "    # Change shape of input from list of values into 28 pixel X 28 pixel X 1 grayscale calue\n",
    "    x_image = tf.reshape(x, [-1, image_height, image_width, 1], name=\"x_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv1'):\n",
    "    # 32 features for each 5X5 patch of the image\n",
    "    W_conv1 = weights([5,5,1,32], name=\"weights\")\n",
    "    b_conv1 = bias([32], name=\"bias\")\n",
    "    # Do convolution on images, add bias and push through RELU activation\n",
    "    h_conv1 = tf.nn.relu(conv(x_image, W_conv1) + b_conv1, name=\"relu\")\n",
    "    # take results and run through max_pool\n",
    "    h_pool1 = max_pool(h_conv1, name=\"pool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv2'):\n",
    "    # Process the 32 features from Convolution layer 1, in 5 X 5 patch.  \n",
    "    # Return 64 features weights and biases\n",
    "    W_conv2 = weights([5,5,32,64], name=\"weights\")\n",
    "    b_conv2 = bias([64], name=\"bias\")\n",
    "    # Do convolution of the output of the 1st convolution layer.  Pool results \n",
    "    h_conv2 = tf.nn.relu(conv(h_pool1, W_conv2) + b_conv2, name=\"relu\")\n",
    "    h_pool2 = max_pool(h_conv2, name=\"pool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected layer with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('FC2'):    \n",
    "    # Fully connected layer\n",
    "    W_fc1 = weights([7*7*64, 1024], name=\"weights\")\n",
    "    b_fc1 = bias([1024], name=\"bias\")\n",
    "    \n",
    "    # Connect output of 2 pooling layer as input to fully connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    z_fc1 = tf.matmul(h_pool2_flat, W_fc1) + b_fc1\n",
    "    \n",
    "    # Batch normalization on FC1 layer \n",
    "    batch_mean, batch_var = tf.nn.moments(z_fc1,[0])\n",
    "    scale = tf.Variable(tf.ones([1024]))\n",
    "    beta = tf.Variable(tf.ones([1024]))\n",
    "    z_fc1_batch_norm = tf.nn.batch_normalization(z_fc1,batch_mean,batch_var,beta,scale,epsilon)\n",
    "    \n",
    "    h_fc1 = tf.nn.relu(z_fc1_batch_norm, name=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Output\"):\n",
    "    # Output layer\n",
    "    W_fc2 = weights([1024, 10])\n",
    "    b_fc2 = bias([10])\n",
    "\n",
    "# Define output \n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function and optimization alghoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss measurement\n",
    "with tf.name_scope(\"cross_entropy\"):  \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "\n",
    "# Optimization    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variable for accuracy measurment and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    # What is correct\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "    # How accurate is it?\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Define predicted values\n",
    "predictions = tf.argmax(y_conv,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#  define number of steps and how often we display progress\n",
    "mini_batch_size = 100\n",
    "num_steps = x_train.shape[0]//mini_batch_size\n",
    "display_every = 100\n",
    "\n",
    "# Create input object which reads data from MNIST datasets. \n",
    "# This dataset will be used as validation set \n",
    "# Perform one-hot encoding to define the digit\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "\n",
    "# Perform training\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch {0}\".format(epoch))\n",
    "    for i in range(num_steps):\n",
    "        start = i * mini_batch_size\n",
    "        end = start + mini_batch_size\n",
    "        x_batch = x_train[start:end, :]\n",
    "        y_batch = y_train[start:end, :]\n",
    "        train_step.run(feed_dict={x: x_batch, y: y_batch, keep_prob: 0.7})\n",
    "    \n",
    "        # Periodic status display\n",
    "        if i%display_every == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:x_batch, y: y_batch, keep_prob: 1.0})\n",
    "            end_time = time.time()\n",
    "            print(\"step {0}, elapsed time {1:.2f} seconds, training accuracy {2:.3f}%\".format(i, end_time-start_time, train_accuracy*100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to train\n",
    "end_time = time.time()\n",
    "print(\"Total training time: {0:.1f} seconds\".format(end_time-start_time))\n",
    "\n",
    "# Accuracy on validation set data\n",
    "print(\"Validation set accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array of results    \n",
    "y_test = np.zeros((x_test.shape[0]))\n",
    "num_steps = x_test.shape[0]//mini_batch_size\n",
    "\n",
    "# Get predictions for test set\n",
    "for i in range(num_steps):\n",
    "    start = i * mini_batch_size\n",
    "    end = start + mini_batch_size\n",
    "    y_test[start:end] = predictions.eval(feed_dict={x: x_test[start:end,:], keep_prob: 1})\n",
    "\n",
    "# Close session    \n",
    "sess.close()\n",
    "# Save predictions\n",
    "save_results(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
